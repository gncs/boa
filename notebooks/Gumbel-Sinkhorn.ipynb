{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tfl = tf.keras.layers\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_sample(shape, eps=1e-20):\n",
    "    \n",
    "    u = tf.random.uniform(shape, minval=0., maxval=1., dtype=tf.float32)\n",
    "    \n",
    "    return -tf.math.log(-tf.math.log(u + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinkhornNormalization(tfl.Layer):\n",
    "    \n",
    "    def __init__(self, num_samples=1, temperature=0.1, iters=20, noise=0.1, name=\"sinkhorn_layer\", **kwargs):\n",
    "        \n",
    "        super(SinkhornNormalization, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.num_samples = num_samples\n",
    "        self.iters = iters\n",
    "        self.noise = noise\n",
    "        self.temperature = tf.Variable(temperature, dtype=tf.float32, trainable=False, name=\"temperature\")\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        \n",
    "        n = inputs.shape[1]\n",
    "        \n",
    "        \n",
    "        # Reshape to batch of square matrices\n",
    "        inputs = tf.reshape(inputs, [-1, n, n])\n",
    "        batch_size = inputs.shape[0]\n",
    "        \n",
    "        inputs = tf.tile(inputs, [self.num_samples if training else 1, 1, 1])\n",
    "        \n",
    "        inputs = inputs + gumbel_sample(inputs.shape) * self.noise\n",
    "        \n",
    "        inputs = inputs / self.temperature\n",
    "    \n",
    "        for _ in range(self.iters):\n",
    "            \n",
    "            # row normalization\n",
    "            inputs -= tf.reshape(tf.reduce_logsumexp(inputs, axis=1), [-1, 1, n])\n",
    "            \n",
    "            # column normalization\n",
    "            inputs -= tf.reshape(tf.reduce_logsumexp(inputs, axis=2), [-1, n, 1])\n",
    "            \n",
    "        soft_perm = tf.exp(inputs)\n",
    "        \n",
    "        soft_perm = tf.reshape(soft_perm, [-1, batch_size, n, n])\n",
    "        soft_perm = tf.transpose(soft_perm, [1, 0, 2, 3])\n",
    "        \n",
    "        return soft_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SortingNetwork(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 units, \n",
    "                 output_size, \n",
    "                 temperature=0.5, \n",
    "                 iters=5, \n",
    "                 num_gumbel_samples=1,\n",
    "                 noise=0.1,\n",
    "                 name=\"sorting_network\", \n",
    "                 **kwargs):\n",
    "        \n",
    "        super(SortingNetwork, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.units = units\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Create Layers\n",
    "        \n",
    "        self.layer1 = tfl.Dense(self.units,\n",
    "                                activation=\"relu\")\n",
    "        \n",
    "        self.layer2 = tfl.Dense(self.output_size,\n",
    "                                activation=None)\n",
    "        \n",
    "        self.sinkhorn_layer = SinkhornNormalization(temperature=temperature, \n",
    "                                                    iters=iters,\n",
    "                                                    num_samples=num_gumbel_samples,\n",
    "                                                    noise=noise)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        \n",
    "        n = inputs.shape[1]\n",
    "        inputs = tf.reshape(inputs, [-1, n])\n",
    "        \n",
    "        batch_size = inputs.shape[0]\n",
    "        \n",
    "        flattened_inputs = tf.reshape(inputs, [-1, 1])\n",
    "        \n",
    "        activations = self.layer1(flattened_inputs)\n",
    "        activations = self.layer2(activations)\n",
    "        \n",
    "        activations = tf.reshape(activations, [batch_size, self.output_size, self.output_size])\n",
    "        \n",
    "        soft_perm = self.sinkhorn_layer(activations, training=training)\n",
    "\n",
    "        return soft_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sorting_dataset(num_examples=1000, size=10, lower=0., upper=1., batch_size=32, shuffle_buffer=1000):\n",
    "    \n",
    "    shuffled = tf.random.uniform(shape=(num_examples, size),\n",
    "                                 minval=lower,\n",
    "                                 maxval=upper)\n",
    "    \n",
    "    sort = tf.sort(shuffled, axis=1)\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices((shuffled, sort))\n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.repeat()\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter: 1000, loss: 0.035600218921899796\n",
      "counter: 2000, loss: 0.020456058904528618\n",
      "counter: 3000, loss: 0.01786045916378498\n",
      "counter: 4000, loss: 0.010079711675643921\n",
      "counter: 5000, loss: 0.008389798924326897\n",
      "counter: 6000, loss: 0.005025537684559822\n",
      "counter: 7000, loss: 0.0038380285259336233\n",
      "counter: 8000, loss: 0.003997307736426592\n",
      "counter: 9000, loss: 0.0036740736104547977\n",
      "counter: 10000, loss: 0.0024839150719344616\n"
     ]
    }
   ],
   "source": [
    "train_iters = 10000\n",
    "output_size = 5\n",
    "noise=0.1\n",
    "num_samples=10\n",
    "\n",
    "optimizer = tf.optimizers.Adam(1e-4)\n",
    "\n",
    "dataset = create_sorting_dataset(num_examples=10000,\n",
    "                                 size=output_size,\n",
    "                                 batch_size=num_samples)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "sorting_net = SortingNetwork(units=32, \n",
    "                             output_size=output_size, \n",
    "                             noise=noise, \n",
    "                             num_gumbel_samples=num_samples)\n",
    "\n",
    "for shuffled, sort in dataset.take(train_iters):\n",
    "    counter += 1\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        soft_perms = sorting_net(shuffled, training=True)\n",
    "        \n",
    "        tiled_shuffled = tf.reshape(tf.tile(shuffled, [num_samples, 1]), [num_samples, num_samples, output_size])\n",
    "        tiled_shuffled = tf.transpose(tiled_shuffled, [1, 0, 2])\n",
    "        \n",
    "        tiled_sorted = tf.reshape(tf.tile(sort, [num_samples, 1]), [num_samples, num_samples, output_size])\n",
    "        tiled_sorted = tf.transpose(tiled_sorted, [1, 0, 2])\n",
    "        \n",
    "        inv_soft_perms = tf.transpose(soft_perms, [0, 1, 3, 2])\n",
    "        \n",
    "        tiled_unshuffled = tf.einsum(\"ijkl, ijl -> ijk\", inv_soft_perms, tiled_shuffled)\n",
    "        \n",
    "        loss = tf.reduce_mean(tf.math.squared_difference(tiled_unshuffled, tiled_sorted))\n",
    "        \n",
    "    gradients = tape.gradient(loss, sorting_net.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, sorting_net.trainable_variables))\n",
    "    \n",
    "    if counter % 1000 == 0:\n",
    "        print(f\"counter: {counter}, loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 3 2 1 0]\n",
      "[[4. 3. 2. 1. 0.]]\n",
      "[3.0262067e-04 6.5753275e-01 2.2717445e+00 3.5118790e+00 3.5585406e+00]\n",
      "[0. 1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[4, 3, 2, 1, 0]], dtype=tf.float32)\n",
    "\n",
    "soft_perm = tf.squeeze(sorting_net(x, training=False))\n",
    "perm = tf.argmax(soft_perm, axis=1).numpy()\n",
    "\n",
    "print(perm)\n",
    "print(x.numpy())\n",
    "print(tf.matmul(x, soft_perm)[0].numpy())\n",
    "print(x.numpy()[0, perm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
