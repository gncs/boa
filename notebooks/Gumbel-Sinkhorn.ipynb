{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tfl = tf.keras.layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_sample(shape, eps=1e-20):\n",
    "    \n",
    "    u = tf.random.uniform(shape, minval=0., maxval=1., dtype=tf.float32)\n",
    "    \n",
    "    return -tf.math.log(-tf.math.log(u + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinkhornNormalization(tfl.Layer):\n",
    "    \n",
    "    def __init__(self, num_samples=1, temperature=0.1, iters=20, noise=0.1, name=\"sinkhorn_layer\", **kwargs):\n",
    "        \n",
    "        super(SinkhornNormalization, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.num_samples = num_samples\n",
    "        self.iters = iters\n",
    "        self.noise = noise\n",
    "        self.temperature = tf.Variable(temperature, dtype=tf.float32, trainable=False, name=\"temperature\")\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        \n",
    "        n = inputs.shape[1]\n",
    "        \n",
    "        \n",
    "        # Reshape to batch of square matrices\n",
    "        inputs = tf.reshape(inputs, [-1, n, n])\n",
    "        batch_size = inputs.shape[0]\n",
    "        \n",
    "        inputs = tf.tile(inputs, [self.num_samples if training else 1, 1, 1])\n",
    "        \n",
    "        inputs = inputs + gumbel_sample(inputs.shape) * self.noise\n",
    "        \n",
    "        inputs = inputs / self.temperature\n",
    "    \n",
    "        for _ in range(self.iters):\n",
    "            \n",
    "            # row normalization\n",
    "            inputs -= tf.reshape(tf.reduce_logsumexp(inputs, axis=1), [-1, 1, n])\n",
    "            \n",
    "            # column normalization\n",
    "            inputs -= tf.reshape(tf.reduce_logsumexp(inputs, axis=2), [-1, n, 1])\n",
    "            \n",
    "        soft_perm = tf.exp(inputs)\n",
    "        \n",
    "        soft_perm = tf.reshape(soft_perm, [-1, batch_size, n, n])\n",
    "        soft_perm = tf.transpose(soft_perm, [1, 0, 2, 3])\n",
    "        \n",
    "        return soft_perm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SortingNetwork(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 units, \n",
    "                 output_size, \n",
    "                 temperature=0.5, \n",
    "                 iters=5, \n",
    "                 num_gumbel_samples=1,\n",
    "                 noise=0.1,\n",
    "                 name=\"sorting_network\", \n",
    "                 **kwargs):\n",
    "        \n",
    "        super(SortingNetwork, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.units = units\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Create Layers\n",
    "        \n",
    "        self.layer1 = tfl.Dense(self.units,\n",
    "                                activation=\"relu\")\n",
    "        \n",
    "        self.layer2 = tfl.Dense(self.output_size,\n",
    "                                activation=None)\n",
    "        \n",
    "        self.sinkhorn_layer = SinkhornNormalization(temperature=temperature, \n",
    "                                                    iters=iters,\n",
    "                                                    num_samples=num_gumbel_samples,\n",
    "                                                    noise=noise)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        \n",
    "        n = inputs.shape[1]\n",
    "        inputs = tf.reshape(inputs, [-1, n])\n",
    "        \n",
    "        batch_size = inputs.shape[0]\n",
    "        \n",
    "        flattened_inputs = tf.reshape(inputs, [-1, 1])\n",
    "        \n",
    "        activations = self.layer1(flattened_inputs)\n",
    "        activations = self.layer2(activations)\n",
    "        \n",
    "        activations = tf.reshape(activations, [batch_size, self.output_size, self.output_size])\n",
    "        \n",
    "        soft_perm = self.sinkhorn_layer(activations, training=training)\n",
    "\n",
    "        return soft_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sorting_dataset(num_examples=1000, size=10, lower=0., upper=1., batch_size=32, shuffle_buffer=1000):\n",
    "    \n",
    "    shuffled = tf.random.uniform(shape=(num_examples, size),\n",
    "                                 minval=lower,\n",
    "                                 maxval=upper)\n",
    "    \n",
    "    sort = tf.sort(shuffled, axis=1)\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices((shuffled, sort))\n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.repeat()\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56520df8e5e44b858f1d7347a2b99049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter: 1000, loss: 0.07081587612628937\n",
      "counter: 2000, loss: 0.04847860336303711\n",
      "counter: 3000, loss: 0.025779765099287033\n",
      "counter: 4000, loss: 0.02271021530032158\n",
      "counter: 5000, loss: 0.017460448667407036\n",
      "counter: 6000, loss: 0.015931902453303337\n",
      "counter: 7000, loss: 0.013747588731348515\n",
      "counter: 8000, loss: 0.01528224628418684\n",
      "counter: 9000, loss: 0.01077294535934925\n",
      "counter: 10000, loss: 0.012659034691751003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_iters = 10000\n",
    "output_size = 6\n",
    "noise=0.5\n",
    "num_samples=10\n",
    "temperature=0.3\n",
    "\n",
    "optimizer = tf.optimizers.Adam(1e-4)\n",
    "\n",
    "dataset = create_sorting_dataset(num_examples=50000,\n",
    "                                 size=output_size,\n",
    "                                 batch_size=num_samples)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "sorting_net = SortingNetwork(units=32, \n",
    "                             output_size=output_size, \n",
    "                             noise=noise, \n",
    "                             num_gumbel_samples=num_samples,\n",
    "                             temperature=temperature)\n",
    "\n",
    "for shuffled, sort in tqdm(dataset.take(train_iters), total=train_iters):\n",
    "    counter += 1\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        soft_perms = sorting_net(shuffled, training=True)\n",
    "        \n",
    "        tiled_shuffled = tf.reshape(tf.tile(shuffled, [num_samples, 1]), [num_samples, num_samples, output_size])\n",
    "        tiled_shuffled = tf.transpose(tiled_shuffled, [1, 0, 2])\n",
    "        \n",
    "        tiled_sorted = tf.reshape(tf.tile(sort, [num_samples, 1]), [num_samples, num_samples, output_size])\n",
    "        tiled_sorted = tf.transpose(tiled_sorted, [1, 0, 2])\n",
    "        \n",
    "        inv_soft_perms = tf.transpose(soft_perms, [0, 1, 3, 2])\n",
    "        \n",
    "        tiled_unshuffled = tf.einsum(\"ijkl, ijl -> ijk\", inv_soft_perms, tiled_shuffled)\n",
    "        \n",
    "        loss = tf.reduce_mean(tf.math.squared_difference(tiled_unshuffled, tiled_sorted))\n",
    "        \n",
    "    gradients = tape.gradient(loss, sorting_net.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, sorting_net.trainable_variables))\n",
    "    \n",
    "    if counter % 1000 == 0:\n",
    "        print(f\"counter: {counter}, loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0 5 1 3 2]\n",
      "[[0.7391434  0.13792253 0.9787675  0.22269034 0.806396   0.36929846]]\n",
      "[0.18392637 0.4347278  0.40168625 0.78888756 0.8034184  0.6415718 ]\n",
      "[0.806396   0.7391434  0.36929846 0.13792253 0.22269034 0.9787675 ]\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 6))\n",
    "\n",
    "soft_perm = tf.squeeze(sorting_net(x, training=False))\n",
    "perm = tf.argmax(soft_perm, axis=1).numpy()\n",
    "\n",
    "print(perm)\n",
    "print(x.numpy())\n",
    "print(tf.matmul(x, soft_perm)[0].numpy())\n",
    "print(x.numpy()[0, perm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
